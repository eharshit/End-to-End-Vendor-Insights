{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0989b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Make sure 'logs/' folder exists\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    filename=\"logs/ingestion_db.log\",\n",
    "    level=logging.INFO,  # make sure it's INFO or DEBUG\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    filemode=\"a\"\n",
    ")\n",
    "\n",
    "engine = create_engine('sqlite:///inventory.db')\n",
    "\n",
    "def ingest_db(df, table_name, engine):\n",
    "    try:\n",
    "        df.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "        logging.info(f\"Uploaded: {table_name} | Shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to ingest {table_name}: {e}\")\n",
    "\n",
    "def ingest_large_csv(file_path, table_name, engine, chunk_size=100_000):\n",
    "    try:\n",
    "        for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
    "            mode = 'replace' if i == 0 else 'append'\n",
    "            chunk.to_sql(table_name, con=engine, if_exists=mode, index=False)\n",
    "            logging.info(f\"{table_name} - Chunk {i+1} inserted: {chunk.shape}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to ingest large file {table_name}: {e}\")\n",
    "\n",
    "def load_raw_data():\n",
    "    start = time.time()\n",
    "    data_folder = 'data'\n",
    "\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(data_folder, file)\n",
    "            table_name = file[:-4]\n",
    "\n",
    "            if file == 'vendor_invoice.csv':\n",
    "                ingest_large_csv(file_path, table_name, engine)\n",
    "            else:\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    ingest_db(df, table_name, engine)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed to read {file}: {e}\")\n",
    "\n",
    "    end = time.time()\n",
    "    total_time = (end - start) / 60\n",
    "    logging.info(\"-------------Ingestion Complete-------------\")\n",
    "    logging.info(f\"Total Time Taken: {total_time:.2f} minutes\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    load_raw_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6afb5223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: begin_inventory | Shape: (206529, 9)\n",
      "Uploaded: end_inventory | Shape: (224489, 9)\n",
      "Chunk loading heavy file: purchases.csv\n",
      "purchases - Chunk 1 inserted: (100000, 16)\n",
      "purchases - Chunk 2 inserted: (100000, 16)\n",
      "purchases - Chunk 3 inserted: (100000, 16)\n",
      "purchases - Chunk 4 inserted: (100000, 16)\n",
      "purchases - Chunk 5 inserted: (100000, 16)\n",
      "purchases - Chunk 6 inserted: (100000, 16)\n",
      "purchases - Chunk 7 inserted: (100000, 16)\n",
      "purchases - Chunk 8 inserted: (100000, 16)\n",
      "purchases - Chunk 9 inserted: (100000, 16)\n",
      "purchases - Chunk 10 inserted: (100000, 16)\n",
      "purchases - Chunk 11 inserted: (100000, 16)\n",
      "purchases - Chunk 12 inserted: (100000, 16)\n",
      "purchases - Chunk 13 inserted: (100000, 16)\n",
      "purchases - Chunk 14 inserted: (100000, 16)\n",
      "purchases - Chunk 15 inserted: (100000, 16)\n",
      "purchases - Chunk 16 inserted: (100000, 16)\n",
      "purchases - Chunk 17 inserted: (100000, 16)\n",
      "purchases - Chunk 18 inserted: (100000, 16)\n",
      "purchases - Chunk 19 inserted: (100000, 16)\n",
      "purchases - Chunk 20 inserted: (100000, 16)\n",
      "purchases - Chunk 21 inserted: (100000, 16)\n",
      "purchases - Chunk 22 inserted: (100000, 16)\n",
      "purchases - Chunk 23 inserted: (100000, 16)\n",
      "purchases - Chunk 24 inserted: (72474, 16)\n",
      "Uploaded: purchase_prices | Shape: (12261, 9)\n",
      "Chunk loading heavy file: sales.csv\n",
      "sales - Chunk 1 inserted: (100000, 14)\n",
      "sales - Chunk 2 inserted: (100000, 14)\n",
      "sales - Chunk 3 inserted: (100000, 14)\n",
      "sales - Chunk 4 inserted: (100000, 14)\n",
      "sales - Chunk 5 inserted: (100000, 14)\n",
      "sales - Chunk 6 inserted: (100000, 14)\n",
      "sales - Chunk 7 inserted: (100000, 14)\n",
      "sales - Chunk 8 inserted: (100000, 14)\n",
      "sales - Chunk 9 inserted: (100000, 14)\n",
      "sales - Chunk 10 inserted: (100000, 14)\n",
      "sales - Chunk 11 inserted: (100000, 14)\n",
      "sales - Chunk 12 inserted: (100000, 14)\n",
      "sales - Chunk 13 inserted: (100000, 14)\n",
      "sales - Chunk 14 inserted: (100000, 14)\n",
      "sales - Chunk 15 inserted: (100000, 14)\n",
      "sales - Chunk 16 inserted: (100000, 14)\n",
      "sales - Chunk 17 inserted: (100000, 14)\n",
      "sales - Chunk 18 inserted: (100000, 14)\n",
      "sales - Chunk 19 inserted: (100000, 14)\n",
      "sales - Chunk 20 inserted: (100000, 14)\n",
      "sales - Chunk 21 inserted: (100000, 14)\n",
      "sales - Chunk 22 inserted: (100000, 14)\n",
      "sales - Chunk 23 inserted: (100000, 14)\n",
      "sales - Chunk 24 inserted: (100000, 14)\n",
      "sales - Chunk 25 inserted: (100000, 14)\n",
      "sales - Chunk 26 inserted: (100000, 14)\n",
      "sales - Chunk 27 inserted: (100000, 14)\n",
      "sales - Chunk 28 inserted: (100000, 14)\n",
      "sales - Chunk 29 inserted: (100000, 14)\n",
      "sales - Chunk 30 inserted: (100000, 14)\n",
      "sales - Chunk 31 inserted: (100000, 14)\n",
      "sales - Chunk 32 inserted: (100000, 14)\n",
      "sales - Chunk 33 inserted: (100000, 14)\n",
      "sales - Chunk 34 inserted: (100000, 14)\n",
      "sales - Chunk 35 inserted: (100000, 14)\n",
      "sales - Chunk 36 inserted: (100000, 14)\n",
      "sales - Chunk 37 inserted: (100000, 14)\n",
      "sales - Chunk 38 inserted: (100000, 14)\n",
      "sales - Chunk 39 inserted: (100000, 14)\n",
      "sales - Chunk 40 inserted: (100000, 14)\n",
      "sales - Chunk 41 inserted: (100000, 14)\n",
      "sales - Chunk 42 inserted: (100000, 14)\n",
      "sales - Chunk 43 inserted: (100000, 14)\n",
      "sales - Chunk 44 inserted: (100000, 14)\n",
      "sales - Chunk 45 inserted: (100000, 14)\n",
      "sales - Chunk 46 inserted: (100000, 14)\n",
      "sales - Chunk 47 inserted: (100000, 14)\n",
      "sales - Chunk 48 inserted: (100000, 14)\n",
      "sales - Chunk 49 inserted: (100000, 14)\n",
      "sales - Chunk 50 inserted: (100000, 14)\n",
      "sales - Chunk 51 inserted: (100000, 14)\n",
      "sales - Chunk 52 inserted: (100000, 14)\n",
      "sales - Chunk 53 inserted: (100000, 14)\n",
      "sales - Chunk 54 inserted: (100000, 14)\n",
      "sales - Chunk 55 inserted: (100000, 14)\n",
      "sales - Chunk 56 inserted: (100000, 14)\n",
      "sales - Chunk 57 inserted: (100000, 14)\n",
      "sales - Chunk 58 inserted: (100000, 14)\n",
      "sales - Chunk 59 inserted: (100000, 14)\n",
      "sales - Chunk 60 inserted: (100000, 14)\n",
      "sales - Chunk 61 inserted: (100000, 14)\n",
      "sales - Chunk 62 inserted: (100000, 14)\n",
      "sales - Chunk 63 inserted: (100000, 14)\n",
      "sales - Chunk 64 inserted: (100000, 14)\n",
      "sales - Chunk 65 inserted: (100000, 14)\n",
      "sales - Chunk 66 inserted: (100000, 14)\n",
      "sales - Chunk 67 inserted: (100000, 14)\n",
      "sales - Chunk 68 inserted: (100000, 14)\n",
      "sales - Chunk 69 inserted: (100000, 14)\n",
      "sales - Chunk 70 inserted: (100000, 14)\n",
      "sales - Chunk 71 inserted: (100000, 14)\n",
      "sales - Chunk 72 inserted: (100000, 14)\n",
      "sales - Chunk 73 inserted: (100000, 14)\n",
      "sales - Chunk 74 inserted: (100000, 14)\n",
      "sales - Chunk 75 inserted: (100000, 14)\n",
      "sales - Chunk 76 inserted: (100000, 14)\n",
      "sales - Chunk 77 inserted: (100000, 14)\n",
      "sales - Chunk 78 inserted: (100000, 14)\n",
      "sales - Chunk 79 inserted: (100000, 14)\n",
      "sales - Chunk 80 inserted: (100000, 14)\n",
      "sales - Chunk 81 inserted: (100000, 14)\n",
      "sales - Chunk 82 inserted: (100000, 14)\n",
      "sales - Chunk 83 inserted: (100000, 14)\n",
      "sales - Chunk 84 inserted: (100000, 14)\n",
      "sales - Chunk 85 inserted: (100000, 14)\n",
      "sales - Chunk 86 inserted: (100000, 14)\n",
      "sales - Chunk 87 inserted: (100000, 14)\n",
      "sales - Chunk 88 inserted: (100000, 14)\n",
      "sales - Chunk 89 inserted: (100000, 14)\n",
      "sales - Chunk 90 inserted: (100000, 14)\n",
      "sales - Chunk 91 inserted: (100000, 14)\n",
      "sales - Chunk 92 inserted: (100000, 14)\n",
      "sales - Chunk 93 inserted: (100000, 14)\n",
      "sales - Chunk 94 inserted: (100000, 14)\n",
      "sales - Chunk 95 inserted: (100000, 14)\n",
      "sales - Chunk 96 inserted: (100000, 14)\n",
      "sales - Chunk 97 inserted: (100000, 14)\n",
      "sales - Chunk 98 inserted: (100000, 14)\n",
      "sales - Chunk 99 inserted: (100000, 14)\n",
      "sales - Chunk 100 inserted: (100000, 14)\n",
      "sales - Chunk 101 inserted: (100000, 14)\n",
      "sales - Chunk 102 inserted: (100000, 14)\n",
      "sales - Chunk 103 inserted: (100000, 14)\n",
      "sales - Chunk 104 inserted: (100000, 14)\n",
      "sales - Chunk 105 inserted: (100000, 14)\n",
      "sales - Chunk 106 inserted: (100000, 14)\n",
      "sales - Chunk 107 inserted: (100000, 14)\n",
      "sales - Chunk 108 inserted: (100000, 14)\n",
      "sales - Chunk 109 inserted: (100000, 14)\n",
      "sales - Chunk 110 inserted: (100000, 14)\n",
      "sales - Chunk 111 inserted: (100000, 14)\n",
      "sales - Chunk 112 inserted: (100000, 14)\n",
      "sales - Chunk 113 inserted: (100000, 14)\n",
      "sales - Chunk 114 inserted: (100000, 14)\n",
      "sales - Chunk 115 inserted: (100000, 14)\n",
      "sales - Chunk 116 inserted: (100000, 14)\n",
      "sales - Chunk 117 inserted: (100000, 14)\n",
      "sales - Chunk 118 inserted: (100000, 14)\n",
      "sales - Chunk 119 inserted: (100000, 14)\n",
      "sales - Chunk 120 inserted: (100000, 14)\n",
      "sales - Chunk 121 inserted: (100000, 14)\n",
      "sales - Chunk 122 inserted: (100000, 14)\n",
      "sales - Chunk 123 inserted: (100000, 14)\n",
      "sales - Chunk 124 inserted: (100000, 14)\n",
      "sales - Chunk 125 inserted: (100000, 14)\n",
      "sales - Chunk 126 inserted: (100000, 14)\n",
      "sales - Chunk 127 inserted: (100000, 14)\n",
      "sales - Chunk 128 inserted: (100000, 14)\n",
      "sales - Chunk 129 inserted: (25363, 14)\n",
      "Chunk loading heavy file: vendor_invoice.csv\n",
      "vendor_invoice - Chunk 1 inserted: (5543, 10)\n",
      "\n",
      "Ingestion done in 3.75 mins\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('sqlite:///inventory.db')\n",
    "\n",
    "def ingest_db(df, table_name, engine):\n",
    "    df.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "    print(f\"Uploaded: {table_name} | Shape: {df.shape}\")\n",
    "\n",
    "def ingest_large_csv(file_path, table_name, engine, chunk_size=100_000):\n",
    "    for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
    "        mode = 'replace' if i == 0 else 'append'\n",
    "        chunk.to_sql(table_name, con=engine, if_exists=mode, index=False)\n",
    "        print(f\"{table_name} - Chunk {i+1} inserted: {chunk.shape}\")\n",
    "\n",
    "def load_raw_data():\n",
    "    start = time.time()\n",
    "    data_folder = 'data'\n",
    "\n",
    "    heavy = {'vendor_invoice.csv', 'sales.csv', 'purchases.csv'}\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file.endswith('.csv'):\n",
    "            path = os.path.join(data_folder, file)\n",
    "            table = file[:-4]\n",
    "            if file in heavy:\n",
    "                print(f\"Chunk loading heavy file: {file}\")\n",
    "                ingest_large_csv(path, table, engine)\n",
    "            else:\n",
    "                df = pd.read_csv(path)\n",
    "                ingest_db(df, table, engine)\n",
    "\n",
    "    dt = (time.time() - start) / 60\n",
    "    print(f\"\\nIngestion done in {dt:.2f} mins\")\n",
    "\n",
    "# Just call load_raw_data() directly\n",
    "load_raw_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf5e6598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables found in inventory.db:\n",
      "['begin_inventory', 'end_inventory', 'purchase_prices', 'purchases', 'sales', 'vendor_invoice']\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import inspect\n",
    "\n",
    "inspector = inspect(engine)\n",
    "tables = inspector.get_table_names()\n",
    "print(\"Tables found in inventory.db:\")\n",
    "print(tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f3816f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           InventoryId  Store  Brand                 Description        Size  \\\n",
      "0  1_HARDERSFIELD_1004      1   1004  Jim Beam w/2 Rocks Glasses       750mL   \n",
      "1  1_HARDERSFIELD_1004      1   1004  Jim Beam w/2 Rocks Glasses       750mL   \n",
      "2  1_HARDERSFIELD_1004      1   1004  Jim Beam w/2 Rocks Glasses       750mL   \n",
      "3  1_HARDERSFIELD_1004      1   1004  Jim Beam w/2 Rocks Glasses       750mL   \n",
      "4  1_HARDERSFIELD_1005      1   1005     Maker's Mark Combo Pack  375mL 2 Pk   \n",
      "\n",
      "   SalesQuantity  SalesDollars  SalesPrice   SalesDate  Volume  \\\n",
      "0              1         16.49       16.49  2024-01-01   750.0   \n",
      "1              2         32.98       16.49  2024-01-02   750.0   \n",
      "2              1         16.49       16.49  2024-01-03   750.0   \n",
      "3              1         14.49       14.49  2024-01-08   750.0   \n",
      "4              2         69.98       34.99  2024-01-09   375.0   \n",
      "\n",
      "   Classification  ExciseTax  VendorNo                   VendorName  \n",
      "0               1       0.79     12546  JIM BEAM BRANDS COMPANY      \n",
      "1               1       1.57     12546  JIM BEAM BRANDS COMPANY      \n",
      "2               1       0.79     12546  JIM BEAM BRANDS COMPANY      \n",
      "3               1       0.79     12546  JIM BEAM BRANDS COMPANY      \n",
      "4               1       0.79     12546  JIM BEAM BRANDS COMPANY      \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM sales LIMIT 5\", con=engine)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69d2f151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin_inventory: 206529 rows\n",
      "end_inventory: 224489 rows\n",
      "purchase_prices: 12261 rows\n",
      "purchases: 2372474 rows\n",
      "sales: 12825363 rows\n",
      "vendor_invoice: 5543 rows\n"
     ]
    }
   ],
   "source": [
    "for table in inspector.get_table_names():\n",
    "    count = pd.read_sql(f\"SELECT COUNT(*) AS rows FROM {table}\", con=engine)\n",
    "    print(f\"{table}: {count['rows'][0]} rows\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
